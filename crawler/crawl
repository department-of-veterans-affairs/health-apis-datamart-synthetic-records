#!/usr/bin/env bash

[ "$DEBUG" == true ] && set -x

cd $(dirname $(readlink -f $0))

CRAWLER_DIR=$(pwd)


#
# We need a certain amount of sensitive information which is kept
# in a secrets file that is not versioned in GitHub.
#
SECRETS=$CRAWLER_DIR/secrets.conf
[ ! -f $SECRETS ] && echo "Missing $SECRETS" && exit 1
. $SECRETS

#
# Secrets
#
[ -z "$TOKEN" ] && echo "TOKEN not defined" && exit 1
[ -z "$LAB_USER_PASSWORD" ] && echo "LAB_USER_PASSWORD not defined" && exit 1
[ -z "$LAB_CLIENT_ID" ] && echo "LAB_CLIENT_ID not defined" && exit 1
[ -z "$LAB_CLIENT_SECRET" ] && echo "LAB_CLIENT_SECRET not defined" && exit 1

#
# Mandatory variables
#
[ -z "$PATIENT_ID" ] && echo "PATIENT_ID not defined" && exit 1

#
# Optional variables
#
[ -z "$ID" ] && ID=sentinel-crawler
[ -z "$HEALTH_APIS_VERSION" ] && HEALTH_APIS_VERSION="latest"
[ -z "$THREADS" ] && THREADS=10
[ -z "$TIMELIMIT" ] && TIMELIMIT=PT8H0M0S
[ -z "$BASE_URL" ] && BASE_URL=https://dev-api.va.gov
[ -z "$API_PATH" ] && API_PATH=/services/fhir/v0/dstu2
# Only Searches
[ -z "$ALLOW_URLS" ] && ALLOW_URLS=".*patient=.*"

SENTINEL_ENV="LAB"
SENTINEL_CRAWLER="gov.va.api.health.dataquery.tests.crawler.UsingOAuthCrawlerTest"

#
# Ensure we have the latest version
#
docker pull vasdvp/health-apis-data-query-tests:$HEALTH_APIS_VERSION

#
# Clean up any cruft containers from previous runs
#
docker rm "$ID" > /dev/null

#
# Run the crawler
#
docker run \
  --init \
  --name="$ID" \
  --network=host \
  vasdvp/health-apis-data-query-tests:$HEALTH_APIS_VERSION \
  test \
  -Dsentinel=$SENTINEL_ENV \
  -Daccess-token=$TOKEN \
  -Dpatient-id=$PATIENT_ID \
  -Dcrawler.threads=$THREADS \
  -Dcrawler.seed="${SEED:-}" \
  -Dcrawler.timelimit=${TIMELIMIT:-} \
  -Dcrawler.url.replace=https://dev-api.va.gov/services/fhir/v0/dstu2 \
  -Dcrawler.allow-query-url-pattern="$ALLOW_URLS" \
  -Dsentinel.argonaut.url=$BASE_URL \
  -Dsentinel.argonaut.api-path=$API_PATH \
  -Dva-oauth-robot.user-password=$LAB_USER_PASSWORD \
  -Dva-oauth-robot.skip-two-factor-authentication=true \
  -Dva-oauth-robot.client-id=$LAB_CLIENT_ID \
  -Dva-oauth-robot.client-secret=$LAB_CLIENT_SECRET \
  -Dva-oauth-robot.base-url=$BASE_URL$API_PATH \
  -Dva-oauth-robot.redirect-url=https://app/after-auth \
  -Dva-oauth-robot.state=unused \
  -Dva-oauth-robot.aud=alec \
  -Dva-oauth-robot.credentials-mode=HEADER \
  -Dva-oauth-robot.credentials-type=ID_ME \
  $SENTINEL_CRAWLER | tee crawl.log

#
# Lots of magic to extract the records as tars from running container and extract
# some metadata files
#
docker commit $ID $ID-files
ICN=$(docker run --rm --entrypoint "" $ID-files find /sentinel/target/ -type d -name "lab-crawl-*" | sed 's/^.*-//')
OUT=lab-crawl-$ICN.tar.gz
docker run --rm --entrypoint "" $ID-files tar -zcC /sentinel/target -f - . > $OUT
docker rmi $ID-files
docker rm $ID
SUMMARY=./lab-crawl-$ICN/summary.csv
tar xf $OUT $SUMMARY
FAILURES=$(grep INVALID $SUMMARY | wc -l)
mv crawl.log ./lab-crawl-$ICN
echo "$FAILURES" > ./lab-crawl-$ICN/failures
echo "$PATIENT_ID" > ./lab-crawl-$ICN/patient-id
echo "$ICN" > ./lab-crawl-$ICN/icn
echo "Results saved to $OUT (summary: $SUMMARY)"
